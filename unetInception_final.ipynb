{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unetInception_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y-4dn4mnsiXX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyBy4_TvmO3A"
      },
      "source": [
        "# **LOCAL ATTRIBUTE SEGMENTATION IN DERMOSCOPIC IMAGES**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N1HjTmRmRPu"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Skin cancer is one of the most common types of cancer in the world. Early detection, by visual analysis of skin lesions, is the critical stage in order to prevent it and to successfully treat it. Dermoscopy is the technique of visually analyzing superficial skin lesions for medical evaluation, studying the color and shape of the mole, as well as the presence and typicality of certain local structures, such as: **milia-like cysts, negative/inverted networks, normal/pigmented networks, streaks and globules**. \n",
        "\n",
        "\n",
        "\n",
        "Due to the increasing capabilities of Machine Learning algorithms, medical diagnostic can nowadays be performed by convolutional neural networks (CNN), thanks to their state-of-the-art performance in pattern recognition tasks.\n",
        "\n",
        "\n",
        "![alt txt](https://github.com/CesarCaramazana/DermoscopicSegmentation/blob/main/images/structures.PNG?raw=true)\n",
        "\n",
        "Images sources: https://dermoscopedia.org\n",
        "\n",
        "\n",
        "The objective of this notebook is the design and implementation of a semantic segmentation algorithm based on a Fully Convolutional Network (FCN) that identifies and classifies at pixel level the local structures (cysts, networks, streaks and globules) present in a pigmented skin lesion. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFw3Jtdsl4l"
      },
      "source": [
        "## Model proposal: U-net (Inception v3) + ASPP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KqyGcV_Z2Jl"
      },
      "source": [
        "From the beginning, **U-Net** was considered as the starting point for our proposal. The original arquitecture was modified in order to fit the requirements of the ISIC dataset (output number of classes and input resolution), and Resnet101 was used as backbone (pre-trained in ImageNet). Additionally, the last volume of the encoder was incorporated an **Atrous Pyramid Pooling block (ASPP)**, from Deeplab v3, with dilation rates 2, 3 and 4, which slighyly improved the results. \n",
        "The last implementation carried out was the replacement of Resnet101 by **Inception v3**, which parallelizes convolutions and significantly reduces the number of parameters.\n",
        "\n",
        "The following figure represents the arquitecture, which is implemented in the \"Arquitecture\" subsection below:\n",
        "\n",
        "![alt txt](https://github.com/CesarCaramazana/DermoscopicSegmentation/blob/main/images/unet_inception.PNG?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-4dn4mnsiXX"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVqI_-zTshR1"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import collections\n",
        "import re\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "import statistics as stats\n",
        "import random\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0gEkD_YtYUQ"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "I have stored the dataset in my personal Drive account, but the ISIC DB 2017 is available here: https://challenge.isic-archive.com/data\n",
        "\n",
        "It should be downloaded and unzipped in a folder named \"db_isic\", which containes subfolders with the data and text files (.txt) with the paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2kc5XECta6h"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/') #Mount Drive in order to read the neccesary files (such as the dataset)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG9rgJpJjv0V"
      },
      "source": [
        "#We haven't used Tensorboard in this project due to an error that occured when we first tried to launch it with Colab.\n",
        "#The problem, now solved, was that Tensorboard doesn't work if Third-party cookies are blocked!\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "log_dir = \"runs/outputs\"\n",
        "writer = SummaryWriter(log_dir=log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PupfvPVX1wni"
      },
      "source": [
        "#Use GPU if available. (Runtime -> Change runtime type -> GPU)\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p9NxiMktuVV"
      },
      "source": [
        "### Auxiliary Functions\n",
        "\n",
        "We define some auxiliary functions that are needed during various stages of the pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0foeE3cWt1j2"
      },
      "source": [
        "def one_hot(mask):\n",
        "  \"\"\"\n",
        "    Ground truth masks are not one-hot encoded in the original ISIC DB.\n",
        "      input: [1, h, w] mask with indices [0, 6].\n",
        "      output: [6, h, w] binary masks.\n",
        "  \"\"\"\n",
        "  \n",
        "  new = np.zeros((6,mask.shape[0],mask.shape[1]))\n",
        "  new[0, (mask==1)] = 1 #Others\n",
        "  new[1, (mask==2)] = 1 #Cysts  \n",
        "  new[2, (mask==3)] = 1 #Neg network \n",
        "  new[3, (mask==4)] = 1 #Pigm network\n",
        "  new[4, (mask==5)] = 1 #Streaks\n",
        "  new[5, (mask==6)] = 1 #Globules\n",
        "  \n",
        "  new = new.transpose(1,2,0)\n",
        "  return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8lP21wQuYc_"
      },
      "source": [
        "def lesion(mask):\n",
        "  \"\"\"\n",
        "    This function generates a binary mask that separates background pixels (healthy skin) and the lesion.\n",
        "    The mask is used to eliminate the maximum amount of background pixels when reading an image from the dataset.\n",
        "  \n",
        "      input: mask(h,w), indices[0,6]: Bg, Others, Cysts...\n",
        "      output: mask'(h,w), indices[0,1]: Bg, Fg \n",
        "  \"\"\"\n",
        "\n",
        "  new = np.zeros(mask.shape)\n",
        "  new[(mask==0)] = 0 \n",
        "  new[(mask==1)] = 1 \n",
        "  new[(mask==2)] = 1  \n",
        "  new[(mask==3)] = 1  \n",
        "  new[(mask==4)] = 1 \n",
        "  new[(mask==5)] = 1 \n",
        "  new[(mask==6)] = 1 \n",
        "\n",
        "  return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rees-vJRvArH"
      },
      "source": [
        "def get_segment_crop(img,tol=0, mask=None):\n",
        "  \"\"\"\n",
        "  This function crops and image given a background-foreground binary mask. \n",
        "    inputs: image [c, h, w]\n",
        "             mask [1, h, w]\n",
        "    output: image [c, h', w'], where h'w' < hw        \n",
        "  \"\"\"\n",
        "  if mask is None:\n",
        "    mask = img > tol\n",
        "  \n",
        "  return img[np.ix_(mask.any(1), mask.any(0))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlU151Gm9eWX"
      },
      "source": [
        "def palette(mask):\n",
        "  \"\"\"\n",
        "    This function defines a color palette for the plots, instead of using the default one.\n",
        "  \"\"\"\n",
        "\n",
        "  red = np.zeros(mask.shape) #Red channel\n",
        "  green = np.zeros(mask.shape) #Green channel\n",
        "  blue = np.zeros(mask.shape) #Blue channel\n",
        "\n",
        "  #Background (dark blue)\n",
        "  red[np.where(mask==0)] = 0.08\n",
        "  green[np.where(mask==0)] = 0.08\n",
        "  blue[np.where(mask==0)] = 0.17\n",
        "\n",
        "  #Foreground (bright grey)\n",
        "  red[np.where(mask==1)] = 0.88\n",
        "  green[np.where(mask==1)] = 0.97\n",
        "  blue[np.where(mask==1)] = 0.98\n",
        "\n",
        "  rgb = np.stack((red,green,blue)) #3, h, w\n",
        "  rgb = rgb.transpose(1,2,0) #h, w, 3\n",
        "\n",
        "  return rgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIyFpiSx9IqK"
      },
      "source": [
        "def plot_images(y_score, labels, foreground):\n",
        "  \"\"\"\n",
        "    This function plots the images of interest (prediction masks and ground truth label)\n",
        "  \"\"\"\n",
        "\n",
        "  dictionary = ['Others', 'Cysts', 'Negative network', 'Pigment network', 'Streaks', 'Globules']\n",
        "  y_score = torch.nn.functional.softmax(y_score, dim=0)\n",
        "  \n",
        "  x, y = torch.max(y_score, dim=0)\n",
        "  y = one_hot(y.squeeze())\n",
        "  y = y*foreground\n",
        "  y = postprocessing(y)  \n",
        "  \n",
        "  plt.figure(figsize=(12, 6)) #Prediction\n",
        "  for i in range(6):   \n",
        "    plt.subplot(1,6,i+1)\n",
        "    plt.title(dictionary[i])\n",
        "    plt.imshow(palette(y[i])) #Show using color palette\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  plt.figure(figsize=(12, 6)) #Ground truth\n",
        "  for i in range(6):\n",
        "    plt.subplot(1,6,i+1)\n",
        "    plt.title(dictionary[i])\n",
        "    plt.imshow(palette(labels[i])) #Show using color palette\n",
        "  plt.show() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndIuZ8uwq3h2"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "The preprocessing of the images consists in three operational blocks: \n",
        "\n",
        "1. Elimination of background pixels. \n",
        "\n",
        "2. Resize + Random low resolution cropping (128x128)*\n",
        "\n",
        "3. Data augmentation (random flips and color jitter). \n",
        "\n",
        "![pre](https://github.com/CesarCaramazana/DermoscopicSegmentation/blob/main/images/preproc_pipeline.png?raw=True)\n",
        "\n",
        "**This is important due to the memory limitations of Google Colab. If run in a local GPU, the resolution should be increased without compromising the batch size.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG7KjwCpw0tb"
      },
      "source": [
        "#ImageNet mean/std \n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "#ISIC DB mean/std \n",
        "mean = [0.1769, 0.1479, 0.1367]\n",
        "std = [0.0352, 0.0378, 0.0417]\n",
        "\n",
        "#These operations are applied to the training set. \n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize([256,], interpolation=transforms.InterpolationMode.NEAREST),\n",
        "     transforms.RandomCrop(size=(128)),\n",
        "     #transforms.RandomRotation(90),\n",
        "     transforms.RandomVerticalFlip(p=0.5), \n",
        "     transforms.RandomHorizontalFlip(p=0.5)\n",
        "    ])\n",
        "\n",
        "#These operations are applied to the validation and test set to evaluate the model.\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize([256,], interpolation=transforms.InterpolationMode.NEAREST),\n",
        "     transforms.RandomCrop(size=256)\n",
        "    ])\n",
        "\n",
        "#Data augmentation in the RGB channels\n",
        "data_aug = transforms.Compose(\n",
        "    [transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.05),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=mean, std=std),\n",
        "     transforms.ToPILImage()\n",
        "     \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px85YRn3tw6N"
      },
      "source": [
        "### ISIC Dataset class and Dataloaders\n",
        "\n",
        "The dataset class is defined to fit the modified version of the ISIC DB, so it may not be directly applied to the vanilla version if downloaded from the ISIC website. The main difference is that the indices of the images/labels are stored in text files that are not included in the original database. \n",
        "\n",
        "ISIC DB is composed of 2750 high resolution dermoscopic images of pigmented skin lesions. Some examples of these are shown below:\n",
        "\n",
        "![isic](https://github.com/CesarCaramazana/DermoscopicSegmentation/blob/main/images/isic.jpg?raw=True)\n",
        "\n",
        "Regarding batch size: we use batch size=24 because of consistency among Colab sessions. The memory limitation doesn't allow a higher batch size, which is encouraged when working with Batch Normalization layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awu0b-oIvbe9"
      },
      "source": [
        "class db_isic_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, transform=None, data_aug=None, set='train'):\n",
        "\n",
        "    self.transform = transform\n",
        "    self.data_aug = data_aug\n",
        "    self.subset = set\n",
        "    self.dataDir = ''\n",
        "    self.imgRoot = ''\n",
        "    self.gtRoot = ''\n",
        "    \n",
        "    #In the folder where the dataset was stored, I defined .txt files with the indices of the images.\n",
        "    #These indices have the format: \"ISIC_0014349\", without the file extension.\n",
        "    \n",
        "    if(self.subset == 'train'):\n",
        "      self.imgRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Training_Data/'\n",
        "      self.gtRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Training_Part2_GroundTruth/gtann/'\n",
        "      self.dataDir = '/content/drive/My Drive/db_isic/train.txt'\n",
        "    \n",
        "    if(self.subset == 'val'):\n",
        "      self.imgRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Validation_Data/'\n",
        "      self.gtRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Validation_Part2_GroundTruth/gtann/'\n",
        "      self.dataDir = '/content/drive/My Drive/db_isic/val.txt'\n",
        "\n",
        "    if(self.subset == 'test'):\n",
        "      self.imgRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Test_v2_Data/'\n",
        "      self.gtRoot = '/content/drive/My Drive/db_isic/ISIC-2017_Test_v2_Part2_GroundTruth/gtann/'\n",
        "      self.dataDir = '/content/drive/My Drive/db_isic/test.txt'\n",
        "\n",
        "\n",
        "    self.data = open(self.dataDir)\n",
        "    self.paths = self.data.readlines()\n",
        "\n",
        "   \n",
        "  def __getitem__(self, index):\n",
        "    imagePath = self.imgRoot + self.paths[index][:-1] + '.jpg'\n",
        "    gtPath =  self.gtRoot + self.paths[index][:-1] + '.mat'\n",
        "\n",
        "    image_t = Image.open(imagePath)\n",
        "    mask = scipy.io.loadmat(gtPath)['smgt']\n",
        "\n",
        "    foreground = lesion(mask) #Get binary foreground mask\n",
        "\n",
        "    mask = get_segment_crop(mask, mask=foreground) #Apply foreground mask to input image and label\n",
        "    image_t = get_segment_crop(np.asarray(image_t), mask=foreground) \n",
        "    foreground = get_segment_crop(foreground, mask=foreground)\n",
        "    \n",
        "    if self.data_aug: #Data augmentation\n",
        "      image_t = self.data_aug(image_t)\n",
        "    \n",
        "    mask_t = one_hot(mask) #One-hot encoding   \n",
        "        \n",
        "    seed = np.random.randint(2147483647) # make a seed with numpy generator \n",
        "    \n",
        "    if self.transform:\n",
        "      random.seed(seed) #Apply the same random operations to mask and image\n",
        "      torch.manual_seed(seed)\n",
        "      image_t = self.transform(image_t)\n",
        "      \n",
        "      random.seed(seed)\n",
        "      torch.manual_seed(seed)\n",
        "      mask_t = self.transform(mask_t)\n",
        "\n",
        "      random.seed(seed)\n",
        "      torch.manual_seed(seed)\n",
        "      foreground = self.transform(foreground)\n",
        "\n",
        "\n",
        "    return image_t, mask_t, foreground\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v78w0gTJwYej"
      },
      "source": [
        "#Dataloaders\n",
        "trainset = db_isic_Dataset(transform=transform, data_aug=data_aug, set='train')\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=24, shuffle=True, num_workers=0) #Batch=32 or higher may generate memory usage problems due to the memory limit of Colab.\n",
        "\n",
        "validationset = db_isic_Dataset(transform=transform_test, set='val')\n",
        "validationloader = torch.utils.data.DataLoader(validationset, batch_size=2, shuffle=True)\n",
        "\n",
        "testset = db_isic_Dataset(transform=transform_test, set='test')\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=2, shuffle=True)\n",
        "\n",
        "data_loaders = {\"train\": trainloader, \"val\": validationloader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msYaSdV6x96Z"
      },
      "source": [
        "### Arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GRHyPRGyA6m"
      },
      "source": [
        "class unet_inception(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    out_channels = 6\n",
        "\n",
        "    #Downloads the pre-trained weights of an Inception v3 model\n",
        "    self.inception = models.inception_v3(pretrained=True)\n",
        "    \n",
        "    #ENCODER\n",
        "    self.l1 = nn.Sequential(*list(self.inception.children())[0:3])\n",
        "    self.l2 = nn.Sequential(*list(self.inception.children())[3:6])\n",
        "    self.l3 = nn.Sequential(*list(self.inception.children())[6:10])\n",
        "\n",
        "    #ASPP layers\n",
        "    self.a1 = nn.Sequential( #Dilation 2\n",
        "        nn.Conv2d(in_channels= 192, out_channels=256, kernel_size=3, stride=1, dilation=2, padding=2),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=2, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.a2 = nn.Sequential( #Dilation 2\n",
        "        nn.Conv2d(in_channels= 192, out_channels=256, kernel_size=3, stride=1, dilation=3, padding=3),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=2, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "\n",
        "    )\n",
        "    self.a3 = nn.Sequential( #Dilation 2\n",
        "        nn.Conv2d(in_channels= 192, out_channels=256, kernel_size=3, stride=1, dilation=4, padding=4),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=2, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "\n",
        "    )\n",
        "\n",
        "    #DECODER\n",
        "    self.l5 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=1056, out_channels=800, kernel_size=3, stride=2, padding=0, output_padding=1),\n",
        "        nn.BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 800, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.l6 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=704, out_channels=704, kernel_size=7, stride=2, padding=0, output_padding=0),\n",
        "        nn.BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout2d(p=0.5),\n",
        "        nn.Conv2d(in_channels= 704, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        \n",
        "    )\n",
        "    self.l7 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=320, out_channels=256, kernel_size=7, stride=2, padding=0, output_padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=256, out_channels=out_channels, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(out_channels), \n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    printShapes = 0 #For debugging purposes\n",
        "    \n",
        "    #Encoder--\n",
        "    l1 = self.l1(input)\n",
        "    if(printShapes): print(\"L1 shape: \", l1.shape)\n",
        "    l2 = self.l2(l1)\n",
        "    if(printShapes): print(\"L2 shape: \", l2.shape)\n",
        "    l3 = self.l3(l2)\n",
        "    if(printShapes): print(\"L3 shape: \", l3.shape)\n",
        "\n",
        "    #Apply Atrous Spatial Pyramid Pooling\n",
        "    a1 = self.a1(l2)\n",
        "    a2 = self.a2(l2)\n",
        "    a3 = self.a3(l2)\n",
        "\n",
        "    if(printShapes): print(\"A1 shape: \", a1.shape)\n",
        "    if(printShapes): print(\"A2 shape: \", a2.shape)\n",
        "    if(printShapes): print(\"A3 shape: \", a3.shape)\n",
        "\n",
        "\n",
        "    a = torch.cat((a1, a2, a3), dim=1) #Generate a single volume.\n",
        "    if(printShapes): print(\"ASPP: \", a.shape)\n",
        "    \n",
        "    x = torch.cat((a, l3), dim=1) #Add layer 3\n",
        "    if(printShapes): print(\"AL3: \", x.shape)\n",
        "\n",
        "\n",
        "    #Decoder--\n",
        "    x = self.l5(x)\n",
        "    if(printShapes): print(\"L5 shape: \", x.shape)\n",
        "    x = torch.cat((x, l2), dim=1) #Skip connection 1\n",
        "    x = self.l6(x)\n",
        "    if(printShapes): print(\"L6 shape: \", x.shape)\n",
        "    x = torch.cat((x, l1), dim=1) #Skip connection 2\n",
        "    x = self.l7(x)\n",
        "    if(printShapes): print(\"L7 output: \", x.shape)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Erf_J3yzBA"
      },
      "source": [
        "#Debug. Testing the shapes of the feature maps given an input resolution\n",
        "\n",
        "model = unet_inception()\n",
        "\n",
        "hw = 256\n",
        "input  = torch.rand((2,3,hw,hw)) #Generate random 3-channel tensor (input image)\n",
        "\n",
        "out = model(input)\n",
        "print(out.shape) #(2, 6, hw, hw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB1f20EqzOZC"
      },
      "source": [
        "### Loss function: (Focal) Cross Entropy with class weights\n",
        "\n",
        "The design of the loss function was approached with the class imbalance problem in mind. We use cross entropy with weights, and apply the Focal Loss operation (https://ieeexplore.ieee.org/document/8237586), although either one of the other should be enough to downweight easy examples.\n",
        "\n",
        "\n",
        "The coefficients are calculated as the inverse number of samples and normalized. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHdm5yWHzR3f"
      },
      "source": [
        "class Focal_CE_weights(nn.Module):\n",
        "  \"\"\"\n",
        "    This loss function is a variation of the cross entropy loss function, which adds class weights (calculated before hand) and\n",
        "    the Focal Loss modification. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Focal_CE_weights, self).__init__()\n",
        "    self.kwargs = kwargs       \n",
        "\n",
        "  def forward(self, inputs, targets, smooth=1, gamma=2):\n",
        "    \"\"\"\n",
        "      inputs: raw output probabilities*. *nn.CrossEntropyLoss already incorporates LogSoftmax()\n",
        "      targets: one-hot encoded Ground Truth mask.\n",
        "          \n",
        "      gamma: focal loss downweighting parameter.\n",
        "    \"\"\"      \n",
        "       \n",
        "    #FOCAL CROSS ENTROPY--- \n",
        "    _, targets = torch.max(targets, dim=1) #[b, 7, h, w] -> [b, 1, h, w] Undo one-hot encoding\n",
        "   \n",
        "    class_weights = torch.tensor([4.7e-3, 0.225, 0.264, 0.02, 0.348, 0.136]) #Calculated as the inverse number of samples and normalized\n",
        "    class_weights = class_weights.to(device) #Weights to GPU\n",
        "\n",
        "    BCE_loss = nn.CrossEntropyLoss(reduction='none', weight=class_weights)(inputs, targets)\n",
        "        \n",
        "    #Focal loss: easily classified pixels have a lesser contribution\n",
        "    pt = torch.exp(-BCE_loss)\n",
        "    F_loss = (1-pt)**gamma * BCE_loss\n",
        "      \n",
        "    return torch.mean(F_loss) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_-UnKu60RV9"
      },
      "source": [
        "### Postprocessing\n",
        "\n",
        "The output masks are postprocessed with morphological operations (erosion + dilation) in order to reduce the amount of False Positives generated by the weighting of the loss function. Square structuring elements are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GATJ0GcS0cDL"
      },
      "source": [
        "def postprocessing(y):\n",
        "  \"\"\"\n",
        "    This function applies morphological operations (erosion + dilation) in batch binary masks. \n",
        "    The structuring elements are squared. The size for each class was obtained by trial and error, maximizing the\n",
        "    Jaccard score of the validation set during training. \n",
        "      input: output predicted masks [6, h, w]\n",
        "      output: processed masks [6, h, w]\n",
        "  \"\"\"\n",
        "  \n",
        "  kernel_size = [1, 21, 21, 15, 5, 15] #[others, cysts, n.net, p.net, str, glob]\n",
        "  for c in range(6):\n",
        "    kernel = np.ones((kernel_size[c],kernel_size[c]), np.uint8)\n",
        "    y[c] = cv.erode(y[c], kernel, iterations=1) #Erosion\n",
        "    y[c] = cv.dilate(y[c], kernel, iterations=1) #Dilation\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6MKuN586BRw"
      },
      "source": [
        "### Evaluation metrics: Area under the curve and Jaccard Score\n",
        "\n",
        "In order to evaluate the quality of an image segmentation model the following metrics are used: area under the (ROC) curve, Jaccard score (IoU) and Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMTyYW-a6F2z"
      },
      "source": [
        "def get_auc_values(y_true, y_score, auc_list, auc_list_epoch):\n",
        "  \"\"\"\n",
        "  Updates the values of the lists \"auc_list\" and \"auc_list_epoch\", which store the AUC values per class.\n",
        "    inputs:\n",
        "      y_true (tensor) [batch, classes, h, w]: ground truth one hot label. \n",
        "      y_score (tensor) [batch, classes, h, w]: output softmax predictions.     \n",
        "  \"\"\"\n",
        "  y_true = y_true.detach()\n",
        "  y_score = y_score.detach()\n",
        "\n",
        "  batch, classes, h, w = y_true.shape\n",
        "\n",
        "  for batch in range(batch):\n",
        "    for c in range(classes):\n",
        "      y_true_v = y_true[batch, c].view(-1, h*w).squeeze() #From [batch, c, h, w] to [h*w] (required for roc_curve())\n",
        "      y_score_v = y_score[batch, c].view(-1, h*w).squeeze()\n",
        "\n",
        "      if len(np.unique(y_true_v)) == 2: #Check if the class is present in the image.\n",
        "        auc_c = roc_auc_score(y_true_v, y_score_v)\n",
        "        auc_list[c].append(auc_c)\n",
        "        auc_list_epoch[c].append(auc_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMeE2NWH6gZz"
      },
      "source": [
        "def get_iou(y_pred, y_true, foreground, iou_list):\n",
        "  \"\"\"\n",
        "  Updates the values of the list \"iou_list\", which stores the jaccard scores per class.\n",
        "    inputs:\n",
        "      y_pred (tensor) [b, 6, h, w],  output probabilities.\n",
        "      y_true (tensor) [b, 6, h, w], Ground Truth labels.\n",
        "      foreground [b, h, w], binary foreground masks.\n",
        "      iou_list [num_classes*[]], list that contains the lists with the IoU values\n",
        "  \"\"\"\n",
        "  batch, classes, h, w = y_true.shape\n",
        "  smooth = 1\n",
        "\n",
        "  _, y_pred = torch.max(y_pred, dim=1) #Classify pixel in class with highest score\n",
        "\n",
        "  y_pred = y_pred.numpy()\n",
        "  y_true = y_true.numpy()\n",
        "  foreground = foreground.numpy()\n",
        "\n",
        "\n",
        "  for batch in range(batch):\n",
        "    y = one_hot(y_pred[batch]) #One-hot output pixel classification (for IoU computation)\n",
        "    y = y * foreground[batch] #Eliminate background (healthy skin) from calculation\n",
        "    y = postprocessing(y) #Postprocess masks\n",
        "    \n",
        "    for c in range(classes):     \n",
        "      intersection = (y[c] * y_true[batch, c]).sum() \n",
        "   \n",
        "      a = (y[c] * y[c]).sum()  #Area of predicted mask\n",
        "      b = (y_true[batch, c] * y_true[batch, c]).sum() #Area of ground truth mask   \n",
        "      iou = intersection / (a+b-intersection + smooth) #IoU\n",
        "\n",
        "      if((a+b) != 0): #Only consider if A or B are not empty\n",
        "        iou_list[c].append(iou)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hg6OFys055K"
      },
      "source": [
        "### Model and parameters\n",
        "\n",
        "We initialize the weights of the ASPP and the decoder layers with Xavier (https://arxiv.org/abs/1704.08863). \n",
        "In order to compensate for the small batch size, we modify the momentum parameter of Batch Normalization layers. \n",
        "\n",
        "Then, establish the learning rate for each layer, the optimizer (Adam), the LR scheduler and the regularization (L2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "covvUvMH08t4"
      },
      "source": [
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "      Defines how weights are initialized in Conv2d and ConvTranspose2d layers. \n",
        "      In this case, using Xavier gaussian distribution.\n",
        "    \"\"\"\n",
        "    #Xavier weight initialization\n",
        "    if isinstance(m, torch.nn.Conv2d):\n",
        "      torch.nn.init.xavier_normal_(m.weight,1.0)\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)\n",
        "    if isinstance(m, torch.nn.ConvTranspose2d):\n",
        "      torch.nn.init.xavier_normal_(m.weight, 1.0)\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLhi-v4d1KnL"
      },
      "source": [
        "def bn_(m):\n",
        "    \"\"\"\n",
        "      Reduce \"momentum\" parameter in BatchNorm2d layers which supposedly works better for small batch size.\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('BatchNorm2d') != -1:\n",
        "        m.momentum = 0.008"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvh0n8J31ZTc"
      },
      "source": [
        "def bn_eval(m):\n",
        "    \"\"\"\n",
        "      Due to an error found during the development of the project, this function needed to be defined \n",
        "      to solve how BatchNorm2d layers work during evaluation. \n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('BatchNorm2d') != -1:\n",
        "        m.track_running_stats = False\n",
        "        m.train() #m.eval() doesn't work properly. The reason is still unknown."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAbh36d22JIG"
      },
      "source": [
        "#ARQUITECTURE\n",
        "model = unet_inception()\n",
        "\n",
        "#We define the parameters of each layer separately to control the Learning Rate and weight initialization.\n",
        "params_e3 = [p for p in model.l3.parameters() if p.requires_grad] #L3 Inception\n",
        "params_a1 = [p for p in model.a1.parameters() if p.requires_grad] #ASPP\n",
        "params_a2 = [p for p in model.a2.parameters() if p.requires_grad]\n",
        "params_a3 = [p for p in model.a3.parameters() if p.requires_grad]\n",
        "params_d5 = [p for p in model.l5.parameters() if p.requires_grad] #Decoder\n",
        "params_d6 = [p for p in model.l6.parameters() if p.requires_grad]\n",
        "params_d7 = [p for p in model.l7.parameters() if p.requires_grad]\n",
        "\n",
        "#Xavier init. DO NOT APPLY TO PRE-TRAINED LAYERS\n",
        "model.a1.apply(weights_init)\n",
        "model.a2.apply(weights_init)\n",
        "model.a3.apply(weights_init)\n",
        "model.l5.apply(weights_init)\n",
        "model.l6.apply(weights_init)\n",
        "model.l7.apply(weights_init)\n",
        "\n",
        "#BatchNorm2d momentum\n",
        "model.apply(bn_)\n",
        "\n",
        "model.to(device) #To GPU or CPU\n",
        "\n",
        "#Optimizer: Adam\n",
        "optimizer = optim.Adam([{'params': params_e3, 'lr': 1e-5}, #Fine-tuning, layer 3\n",
        "                        {'params': params_a1, 'lr': 1e-3},\n",
        "                        {'params': params_a2, 'lr': 1e-3},\n",
        "                        {'params': params_a3, 'lr': 1e-3},\n",
        "                        {'params': params_d5, 'lr': 1e-3},\n",
        "                        {'params': params_d6, 'lr': 1e-3},                         \n",
        "                        {'params': params_d7, 'lr': 1e-3}\n",
        "                        ], lr=1e-3, weight_decay=1e-6) #L2 regularization\n",
        "#Loss function\n",
        "criterion = Focal_CE_weights()\n",
        "\n",
        "#LR scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.95)\n",
        "\n",
        "#Number of epochs\n",
        "num_epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP3UEp_P3qeA"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "The training loop follows a standard structure. To control the progress of the training, we calculate the AUC values for each class, which are stored in two variables: auc_i (with all the values), auc_i_epoch (with the values of the current epoch) to facilitate the evaluation and for academic purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9snBTUsg3sOn"
      },
      "source": [
        "freq = 15 #Plot images every freq iterations\n",
        "\n",
        "checkpoint_path = \"/content/drive/My Drive/Checkpoints/model.tar\" #Load and save model each epoch in a .tar file.\n",
        "\n",
        "saveCheckpoint = True #Saves the model / optimizer each epoch\n",
        "loadCheckpoint = False #Loads the model / optimizer before the loop\n",
        "#---------------------------------------------------------------------------------\n",
        "#We store the loss values in lists. Ideally we would want to use Tensorboard for a \"cleaner\" visualization of the plots. This are mostly for debugging purposes.\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "train_loss_epoch = []\n",
        "val_loss_epoch = []\n",
        "\n",
        "auc_others, auc_cysts, auc_negNet, auc_pigNet, auc_streaks, auc_globules = [], [], [], [], [], []\n",
        "auc_others_epoch, auc_cysts_epoch, auc_negNet_epoch, auc_pigNet_epoch, auc_streaks_epoch, auc_globules_epoch = [], [], [], [], [], []\n",
        "\n",
        "auc_list = [auc_others, auc_cysts, auc_negNet, auc_pigNet, auc_streaks, auc_globules]\n",
        "auc_list_epoch = [auc_others_epoch, auc_cysts_epoch, auc_negNet_epoch, auc_pigNet_epoch, auc_streaks_epoch, auc_globules_epoch]\n",
        "\n",
        "\n",
        "if(loadCheckpoint):\n",
        "  print(\"Loading checkpoint... | Path: \", checkpoint_path)\n",
        "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict']) \n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    if(epoch > 0):       \n",
        "      print(\"__Summary of epoch \", epoch-1)\n",
        "      print(\"|--------------------------------|\")\n",
        "      print(\"| Avg Train Loss       | %.4f  |\" % stats.mean(train_loss_epoch))\n",
        "      print(\"| Avg Val   Loss       | %.4f  |\" % stats.mean(val_loss_epoch))\n",
        "      print(\"|----------------------|---------|\")\n",
        "      print(\"| Avg AUC Other  Val   | %.4f  |\" % stats.mean(auc_others_epoch))\n",
        "      print(\"| Avg AUC M-Cysts  Val | %.4f  |\" % stats.mean(auc_cysts_epoch))\n",
        "      print(\"| Avg AUC Neg net  Val | %.4f  |\" % stats.mean(auc_negNet_epoch))\n",
        "      print(\"| Avg AUC Pig net  Val | %.4f  |\" % stats.mean(auc_pigNet_epoch))\n",
        "      print(\"| Avg AUC Streaks  Val | %.4f  |\" % stats.mean(auc_streaks_epoch))\n",
        "      print(\"| Avg AUC Globules Val | %.4f  |\" % stats.mean(auc_globules_epoch))\n",
        "      print(\"|----------------------|---------|\")\n",
        "\n",
        "      #Tensorboard--\n",
        "      writer.add_scalar(\"Loss/train\", stats.mean(train_loss_epoch), epoch)\n",
        "      writer.add_scalar(\"Loss/validation\", stats.mean(val_loss_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/others\", stats.mean(auc_others_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/cysts\", stats.mean(auc_cysts_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/negNet\", stats.mean(auc_negNet_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/pigNet\", stats.mean(auc_pigNet_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/streaks\", stats.mean(auc_streaks_epoch), epoch)\n",
        "      writer.add_scalar(\"AUC/train/globules\", stats.mean(auc_globules_epoch), epoch)\n",
        "      #--\n",
        "\n",
        "      train_loss_epoch.clear()\n",
        "      val_loss_epoch.clear()\n",
        "      auc_others_epoch.clear()\n",
        "      auc_cysts_epoch.clear()\n",
        "      auc_negNet_epoch.clear()\n",
        "      auc_pigNet_epoch.clear()\n",
        "      auc_streaks_epoch.clear()\n",
        "      auc_globules_epoch.clear()          \n",
        "   \n",
        "      if(saveCheckpoint): #Save model if epoch > 0\n",
        "          print(\"Saving checkpoint... | epoch: \", epoch, \"| Path: \", checkpoint_path)\n",
        "          torch.save({'model_state_dict': model.state_dict(),  \n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state': scheduler.state_dict()},\n",
        "                      checkpoint_path)\n",
        "\n",
        "    \n",
        "   #Training loop--------------------------------------------------------------- \n",
        "    \n",
        "    # Each epoch has a training and a validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "            print(\"Epoch: \", epoch, \"| Training phase |\")\n",
        "        else:\n",
        "            model.eval()  # Set model to evaluate mode\n",
        "            model.apply(bn_eval)\n",
        "            print(\"Epoch: \", epoch, \"| Validation phase |\")\n",
        "\n",
        "        for i, data in enumerate(data_loaders[phase],0):\n",
        "          inputs, labels, foreground = data  \n",
        "          labels = labels.long()\n",
        "          foreground = foreground.long()\n",
        "          foreground = foreground.squeeze(1)  \n",
        "                  \n",
        "          inputs, labels, foreground = inputs.to(device), labels.to(device), foreground.to(device) \n",
        "\n",
        "          #Predict output probabilities\n",
        "          y_score = model(inputs)\n",
        "\n",
        "          #Calculate Loss\n",
        "          loss = criterion(y_score, labels)\n",
        "          \n",
        "          if phase == 'train':            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward() #Backpropagation\n",
        "            optimizer.step()  \n",
        "            scheduler.step() #Sch step\n",
        "\n",
        "            train_loss.append(loss.item()) #Save loss value every iteration*. *For debugging purposes.\n",
        "            train_loss_epoch.append(loss.item())\n",
        "\n",
        "          if phase == 'val':\n",
        "            validation_loss.append(loss.item())\n",
        "            val_loss_epoch.append(loss.item())\n",
        "            get_auc_values(labels.cpu(), y_score.cpu(), auc_list, auc_list_epoch)\n",
        "\n",
        "          \n",
        "          if(i%freq==0):\n",
        "              #Show input image\n",
        "              plt.figure(figsize=(9,9))\n",
        "              plt.title('Input')\n",
        "              plt.imshow(inputs.cpu().data[0].squeeze().permute(1,2,0))                  \n",
        "              plt.show()\n",
        "\n",
        "              #Show predicted and Ground truth masks\n",
        "              plot_images(y_score[0].detach().cpu(), labels[0].cpu(), foreground[0].cpu().numpy())\n",
        "\n",
        "              #Plot loss curves*. In Tensorboard we save only 1 value per epoch. This variables contain the loss values for each iteration. \n",
        "              plt.figure(figsize=(12,7))\n",
        "              plt.subplot(121)\n",
        "              plt.title(\"Training Loss\")\n",
        "              plt.plot(train_loss, 'r')\n",
        "              plt.subplot(122)\n",
        "              plt.title(\"Validation Loss\")\n",
        "              plt.plot(validation_loss, 'b')\n",
        "              plt.show()\n",
        " \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlf4OM1O7wfw"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "This block evaluates the model saved in a .tar file (checkpoint_path) in the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynmMPWPb7yUe"
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Checkpoints/model.tar\"\n",
        "\n",
        "print(\"Loading checkpoint... | Path: \", checkpoint_path)\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "model.apply(bn_eval) #Set BatchNorm2d to evaluation mode\n",
        "\n",
        "#Area under the curve\n",
        "auc_others, auc_cysts, auc_negNet, auc_pigNet, auc_streaks, auc_globules = [], [], [], [], [], []\n",
        "auc_others_epoch, auc_cysts_epoch, auc_negNet_epoch, auc_pigNet_epoch, auc_streaks_epoch, auc_globules_epoch = [], [], [], [], [], []\n",
        "auc_list = [auc_others, auc_cysts, auc_negNet, auc_pigNet, auc_streaks, auc_globules]\n",
        "auc_list_epoch = [auc_others_epoch, auc_cysts_epoch, auc_negNet_epoch, auc_pigNet_epoch, auc_streaks_epoch, auc_globules_epoch]\n",
        "\n",
        "#Jaccard Score\n",
        "iou_others, iou_cysts, iou_negNet, iou_pigNet, iou_streaks, iou_globules = [], [], [], [], [], []\n",
        "iou_list = [iou_others, iou_cysts, iou_negNet, iou_pigNet, iou_streaks, iou_globules]\n",
        "\n",
        "\n",
        "cm_epoch = np.zeros((6,6)) #Confusion Matrix\n",
        "\n",
        "\n",
        "for i, data in enumerate(testloader,0):\n",
        "  inputs, labels, foreground = data  \n",
        "  labels = labels.long()\n",
        "  foreground = foreground.long()\n",
        "  foreground = foreground.squeeze(1)    \n",
        "\n",
        "  inputs, labels, foreground = inputs.to(device), labels.to(device), foreground.to(device) #GPU / CPU  \n",
        "\n",
        "\n",
        "  y_score = model(inputs)\n",
        "  y_score = torch.nn.functional.softmax(y_score, dim=0) #Raw output -> Softmax()\n",
        "  get_iou(y_score.cpu(), labels.cpu(), foreground.cpu(), iou_list)\n",
        "  \n",
        "  x, pred = torch.max(y_score, dim=1) #Classify in highest score class\n",
        "  x2, y = torch.max(labels, dim=1) #Undo one-hot\n",
        "\n",
        "  \n",
        "  prediction = pred.view(-1)  #prediction\n",
        "  yf = y.view(-1) #label\n",
        "  \n",
        "  cm = confusion_matrix(y_true=yf.cpu(), y_pred=prediction.cpu()) #Calculate confusion matrix for the batch\n",
        "  cm_epoch = cm_epoch + cm  #Summatory of the whole test set\n",
        "\n",
        "  #Show input image\n",
        "  plt.figure(figsize=(9,9))              \n",
        "  plt.title('Input')\n",
        "  plt.imshow(inputs.cpu().data[0].squeeze().permute(1,2,0))     \n",
        "  plt.show()\n",
        "    \n",
        "  #Show prediction and Ground truth masks  \n",
        "  plot_images(y_score[0].detach().cpu(), labels[0].cpu(), foreground[0].cpu().numpy())\n",
        "\n",
        "  #Calculate AUC per class\n",
        "  get_auc_values(labels.cpu(), y_score.cpu(), auc_list, auc_list_epoch)\n",
        " \n",
        "print(\"|------------------|---------|\")\n",
        "print(\"| Avg AUC Other    | %.4f  |\" % stats.mean(auc_others_epoch))\n",
        "print(\"| Avg AUC Cysts    | %.4f  |\" % stats.mean(auc_cysts_epoch))\n",
        "print(\"| Avg AUC Neg net  | %.4f  |\" % stats.mean(auc_negNet_epoch))\n",
        "print(\"| Avg AUC Pig net  | %.4f  |\" % stats.mean(auc_pigNet_epoch))\n",
        "print(\"| Avg AUC Streaks  | %.4f  |\" % stats.mean(auc_streaks_epoch))\n",
        "print(\"| Avg AUC Globules | %.4f  |\" % stats.mean(auc_globules_epoch))\n",
        "print(\"|__________________|_________|\")\n",
        "print(\"| Avg IOU Other    | %.4f  |\" % stats.mean(iou_others))\n",
        "print(\"| Avg IOU Cysts    | %.4f  |\" % stats.mean(iou_cysts))\n",
        "print(\"| Avg IOU Neg net  | %.4f  |\" % stats.mean(iou_negNet))\n",
        "print(\"| Avg IOU Pig net  | %.4f  |\" % stats.mean(iou_pigNet))\n",
        "print(\"| Avg IOU Streaks  | %.4f  |\" % stats.mean(iou_streaks))\n",
        "print(\"| Avg IOU Globules | %.4f  |\" % stats.mean(iou_globules))\n",
        "print(\"|__________________|_________|\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIYahuO41BJ1"
      },
      "source": [
        "#Save values in Tensorboard\n",
        "writer.add_scalar(\"AUC/test/others\", stats.mean(auc_others_epoch), 1)\n",
        "writer.add_scalar(\"AUC/test/cysts\", stats.mean(auc_cysts_epoch), 1)\n",
        "writer.add_scalar(\"AUC/test/negNet\", stats.mean(auc_negNet_epoch), 1)\n",
        "writer.add_scalar(\"AUC/test/pigNet\", stats.mean(auc_pigNet_epoch), 1)\n",
        "writer.add_scalar(\"AUC/test/streaks\", stats.mean(auc_streaks_epoch), 1)\n",
        "writer.add_scalar(\"AUC/test/globules\", stats.mean(auc_globules_epoch), 1)\n",
        "\n",
        "writer.add_scalar(\"IOU/test/others\", stats.mean(iou_others), 1)\n",
        "writer.add_scalar(\"IOU/test/cysts\", stats.mean(iou_cysts), 1)\n",
        "writer.add_scalar(\"IOU/test/negNet\", stats.mean(iou_negNet), 1)\n",
        "writer.add_scalar(\"IOU/test/pigNet\", stats.mean(iou_pigNet), 1)\n",
        "writer.add_scalar(\"IOU/test/streaks\", stats.mean(iou_streaks), 1)\n",
        "writer.add_scalar(\"IOU/test/globules\", stats.mean(iou_globules), 1)\n",
        "\n",
        "writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNN3z6UCFVNW"
      },
      "source": [
        "#Print confusion matrix*. *Rows represent the label and columns represent the prediction.\n",
        "print(cm_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZHcYYYgwCXj"
      },
      "source": [
        "## Launch Tensorboard\n",
        "\n",
        "Although the code wasn't developed using Tensorboard, it can be launched by executing the following cell:\n",
        "\n",
        "*There might be some errors since this implementation hasn't been properly debugged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrivdn3DwUrQ"
      },
      "source": [
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3BY_Plt1sAk"
      },
      "source": [
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}